<!DOCTYPE html>
<html>

  ﻿<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Perceptual Foundations of Augmented Reality  (started)</title>
    <meta name="description" content="The Open Augmented Reality Teaching Book">

    <link rel="stylesheet" href="/ar-for-eu-book/css/main.css">
    <link rel="canonical" href="https://klamma.github.io/ar-for-eu-book/chapter/perceptionar/">
    <link rel="alternate" type="application/rss+xml" title="The Open Augmented Reality Teaching Book" href="https://klamma.github.io/ar-for-eu-book/feed.xml">

    <!-- Favicon head tag -->
    <link rel="shortcut icon" href="/ar-for-eu-book/favicon.ico" type="image/x-icon">

    <!-- Visualization Libraries -->
    <script src="https://code.jquery.com/jquery-3.1.1.min.js"></script>
    <script src="https://code.highcharts.com/highcharts.js"></script>
    <script src="https://code.highcharts.com/highcharts-more.js"></script>
    <script src="https://code.highcharts.com/modules/exporting.js"></script>

    <!-- formula rendering -->
    <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/ar-for-eu-book/">The Open Augmented Reality Teaching Book</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
	  <a class="page-link" href="https://codereality.net/">Code Reality</a>
          <a class="page-link" href="/ar-for-eu-book/posts">Blog Posts</a>
          <a class="page-link" href="/ar-for-eu-book/toc">Table of Content</a>
          <a class="page-link" href="/ar-for-eu-book/references">Bibliography</a>
          <a class="page-link" href="/ar-for-eu-book/contrib">Contributors</a>
          <a class="page-link" href="/ar-for-eu-book/about">About</a>
      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Perceptual Foundations of Augmented Reality  (started)</h1>
  </header>

  <div class="post-content">
    
<h1 id="perceptual-foundations-of-augmented-reality">Perceptual Foundations of Augmented Reality</h1>

<p>Definition Perception: The top-down way our brains organize and interpret information and put it into context.</p>

<p>Definition Sensation: The bottom-up process by which our sense, like vision, hearing and touch, receive and relay outside stimuli.</p>

<p><img src="../../assets/figures/perception_ar/No.1%20Definition%20Sensation.JPG" alt="Image of No.1" /></p>

<h2 id="visual-perception">Visual Perception</h2>

<h4 id="what-is-perception">What is perception?</h4>

<p>Perception consists from three main processes called recognition, organizing, and interpreting. Recognition happens basically everyday, we are aware of everything, such as sounds, lights, and colors. Then you are organizing everything that you perceive.  Interpretation refers to the process by which we represent and understand stimuli that affect us.</p>

<p>Recognizing + Organizing+ Interpreting = Sensory Information</p>

<p><img src="../../assets/figures/perception_ar/No.2%20perception.JPG" alt="Image of No.2" /></p>

<h4 id="what-is-visual-perception">What is visual perception?</h4>

<p>Visual perception is the end of product of vision.</p>

<p>“Perception is not something that happens to us, or in us. It is something we do…
Vision is a mode of exploration of the environment drawing on implicit understanding 
of sensorimotor regularities”</p>

<p><img src="../../assets/figures/perception_ar/No.3%20Visual%20perception.jpg" alt="Image of No.3" /></p>

<h4 id="how-vision-works">How vision works?</h4>

<p>The eye passes through the cornea, and go down to the iris. The light reflects and sort of vision is also a chemical reaction.</p>

<p><img src="../../assets/figures/perception_ar/No.4%20vision.JPG" alt="Image of No.4" /></p>

<h4 id="preattentive-features">Preattentive features</h4>

<p>Vision helps us to understand the distinctive features of the environment.</p>

<p><img src="../../assets/figures/perception_ar/No.5%20Preattentive%20features.JPG" alt="Image of No.5" /></p>

<h3 id="gestalt-theory">Gestalt Theory</h3>

<p><img src="../../assets/figures/perception_ar/No.6%20Gestalt%20Theory.JPG" alt="Image of No.6" /></p>

<p><strong>Figure and Ground</strong> explains how we put different elements together to make one scene or a whole image.</p>

<p><img src="../../assets/figures/perception_ar/No.7%20Figure%20and%20Ground.jpg" alt="Image of No.7" /></p>

<p><strong>Similarity</strong> States that things which share visual characteristics such as shape, size, colour, texture, value or orientation will be seen as belonging together.</p>

<p>States that things which share visual characteristics such as shape, size, colour, texture, value or orientation will be seen as belonging together.</p>

<p><img src="../../assets/figures/perception_ar/No.8%20Similarity.jpg" alt="Image of No.8" /></p>

<p><strong>Proximity</strong> Elements tend to be perceived as aggregated into group if they are near each other. other.</p>

<p><img src="../../assets/figures/perception_ar/No.9%20Proximity.jpg" alt="Image of No.9" /></p>

<p><strong>Common Fate</strong>: Objects which are facing the same direction or appear to be travelling in the same direction are usually grouped together.</p>

<p><img src="../../assets/figures/perception_ar/No.10%20Common%20Fate.jpg" alt="Image of No.10" /></p>

<p><strong>Continuity</strong>: in order to fill in missing data we often see things as continuous or whole.</p>

<p><img src="../../assets/figures/perception_ar/No.11%20Continuity.jpg" alt="Image of No.11" /></p>

<p><strong>Closure</strong>: If we have a large pattern with missing components we tend to fill in the missing parts to create the image we actually see.</p>

<p><img src="../../assets/figures/perception_ar/No.12%20Closure.jpg" alt="Image of No.12" /></p>

<p><strong>Area</strong>: When areas are overlapping, the smallest area is seen as the figure and the larger is the ground. When we look at this object we see this as one object on top of another instead of a hole in the larger area.</p>

<p><img src="../../assets/figures/perception_ar/No.13%20Area.JPG" alt="Image of No.13" /></p>

<p><strong>Symmetry</strong>: We tend to organize complex objects into a whole, we are more likely to group symmetrical objects.</p>

<p><img src="../../assets/figures/perception_ar/No.14%20Symmetry.jpg" alt="Image of No.14" /></p>

<p><strong>Vergence-accommodation conflict</strong>: There is a disparity between the physical surface of the screen – accommodation - and the focal point of the simulated world you’re staring at - vergence.</p>

<p><img src="../../assets/figures/perception_ar/No.15%20Vergence-accommodation%20conflict.jpg" alt="Image of No.15" /></p>

<h2 id="spatial-audio">Spatial Audio</h2>

<p>Detecting a weak sensory signal like a random clap in daily life isn’t only about the strength of the 
stimulus. It is also about the psychological state, your alertness and expectations in the moment.</p>

<h4 id="signal-detection-theory-a-model-for-predicting-how-and-when-a-person-will-detect-weak-stimuli-partly-based-on-a-context"><strong>SIGNAL DETECTION THEORY</strong>: a model for predicting how and when a person will detect weak stimuli, partly based on a context.</h4>

<p>Our hearing is adjusted by our preferences by our personal perceptions that is valuable for us.</p>

<p>The paranoid parent’s brains are so trained on their baby, it gives their senses a sort of boosted 
ability, but only in relation to the subject of their attention.</p>

<p><img src="../../assets/figures/perception_ar/No.16%20Spatial%20Audio.jpg" alt="Image of No.16" /></p>

<h2 id="touch">Touch</h2>

<h4 id="proprioception">PROPRIOCEPTION</h4>

<p>Proprioception us the perception of the world  with our body, so how perceive and understand things around us.  our perception embodied and active when we see, we are ready touch, and we tend to visualize our path to the object. Our body experienced all directions of  the environment, we understand the space in 3D. For example, when we see something that we need to grab, we will understand it is a shape.</p>

<p><img src="../../assets/figures/perception_ar/No.17%20Touch.jpg" alt="Image of No.17" /></p>

<h2 id="ui">UI</h2>

<p>UX (User experience) and UI (User interface) is different. <strong>UI</strong> can deal with traditional concepts like visual design elements such as colors and typography. <strong>UX</strong> is all below user interface. User experience design is a human-first way of designing products. Don Norman, a cognitive scientist and co-founder of the Nielsen Norman Group Design Consultancy, is credited with coining the term “user experience.</p>

<p><img src="../../assets/figures/perception_ar/No.18(1)%20UI.jpg" alt="Image of No.18(1)" /></p>

<p>Today UI design is mainly focused on the 2D screens. We are living in 2D world , everyone has a phone, and we use to all references to understand what the camera means, what is this icon.</p>

<p><img src="../../assets/figures/perception_ar/No.18(2)%20UI.JPG" alt="Image of No.18(2)" /></p>

<p><strong>However,</strong> AR (Augmented Reality) points to use all the sense to move away from the screen. In the 60s, the person called Douglas Engelbart, he invented the modern mouse, but the aspect of that not invent something that will constantly use, it was about inventing augment human intelligence that could help us to experience our world in more senses. He wrote articiles about augmenting human intelligence in using all senses that we use our bodies, speech to work with technology.</p>

<p><img src="../../assets/figures/perception_ar/No.18(3)%20UI.JPG" alt="Image of No.18(3)" /></p>

<p>In 1968, Ivan Sutherland who is very famous founder of VR and MR, he said “if the task of the display is to serve as a looking glass into the mathematical wonderland constructed in computer memory, it should serve as many senses as possible”</p>

<p><img src="../../assets/figures/perception_ar/No.18(4)%20UI.gif" alt="Image of No.18(4)" /></p>

<p>In terms of the spatial holograms and spatial interactions, it shows that objects are really realistic that we can work with.</p>

<p><img src="../../assets/figures/perception_ar/No.18(5)%20UI.jpg" alt="Image of No.18(5)" /></p>

<h4 id="affordances">Affordances</h4>

<p>Affordances are the possible actions. Donald Norman (1988) said “possibilities for action that are readily perceivable by individuals”. He invented the storm and created the notion of affordances that means projects represent their functions at 3D world.</p>

<h4 id="ui-in-ar">UI in AR</h4>

<p>The lateral field of human view is about 140 degrees, but with the peripheral visual is 200-220 degrees compared to peripheral vision of the body. We need to follow the natural ways of perception, and don’t put objects on the back bunny.</p>

<p><img src="../../assets/figures/perception_ar/No.19(1)%20Affordances.jpg" alt="Image of No.19(1)" /></p>

<h4 id="the-hierarchy-of-needs-in-ar">The Hierarchy of Needs in AR</h4>

<p><img src="../../assets/figures/perception_ar/No.19(2)%20Affordances.JPG" alt="Image of No.19(2)" /></p>

<h4 id="immersion-and-exploration">Immersion and exploration</h4>

<p>•user is the center of the environment</p>

<p>•users will want to try their own ideas</p>

<p>• scale makes a huge impact on presence</p>

<p>• small details matter</p>

<h4 id="attention">Attention</h4>

<p>•users have freedom to look anywhere, so capturing and guiding attention is important</p>

<p>•audio and visual cues help nudge users in the right way</p>

<p>•forced attention – not always a good idea</p>

<h4 id="ui-toolbox">UI TOOLBOX</h4>

<p><img src="../../assets/figures/perception_ar/No.20(1)%20UI%20TOOLBOX.JPG" alt="Image of No.20(1)" /></p>

<p>We use our gaze (vision) as the way we see things around; we use gaze cursor as the metaphor of the mouse cursor on the computer; Voice can activate different voice commands; We can use spatial audio as 3D emergent audio (an immersion sound) to guide people to show how sounds moved where an object is located by positioning 3D sound in different places; Gestures is how we can use gaze to activate and trigger with these holograms.</p>

<p><img src="../../assets/figures/perception_ar/No.20(2)%20UI%20TOOLBOX.jpg" alt="Image of No.20(2)" /></p>

<p>Note: when you create UI, you need to know:</p>

<p>•Ergonomics</p>

<p>•Safety guidelines for the content</p>

<p>• Clicker, laser pointer</p>

<p>•Other people and their voice commands</p>

<p><strong>Text</strong></p>

<p>— avoid large amounts of text to instruct of inform users with information within an augmented environment</p>

<p>— consider how you will draw attention to text within your environment to capture the user’s attention</p>

<p><strong>Colours</strong></p>

<p>— Shadows and lighting may change the way colour appears on objects and make things.</p>

<p>— By modifying the <strong>saturation</strong> and <strong>brightness</strong> of a single hue, you can generate <strong>multiple colours</strong>— darks, lights, backgrounds, accents, eye-catchers— but it’s not overwhelming on the eye.</p>

<p>— darker colours tend to carry more visual weight, and these kinds of elements need to be balanced out with lighter colours.</p>

<p><strong>The shade of the center dot is the same in all the squares</strong></p>

<p>The shade of the background influences how we perceive it. All squares are uniformly shaded, but each square lighter on its left edge than on its right edge.</p>

<p><img src="../../assets/figures/perception_ar/No.21%20shade.jpg" alt="Image of No.21" /></p>

<h3 id="storytelling">Storytelling</h3>

<p>Storytelling comes from writers, it can be histories, stories of legends.</p>

<h4 id="storyboarding">Storyboarding</h4>

<p>Storyboard is the collection of the shots of the images for movies that will be created.</p>

<p><img src="../../assets/figures/perception_ar/No.22(1)%20Storyboarding.JPG" alt="Image of No.22(1)" /></p>

<p>But in 3D,  the simplest way is to go to the 3D modelling application, such as Unity, Maya, and Cinema 4D, to create a story using prefabs, the 3D models of objects, and explain what is going to happen with the launch examination, and how the user is going to interact with objects.</p>

<p><img src="../../assets/figures/perception_ar/No.22(2)%20Storyboarding.jpg" alt="Image of No.22(2)" /></p>

<h3 id="map-your-story">Map your story</h3>

<p>This will be an inherently interdisciplinary endeavour, and it will need contributions from designers, data engineers and visualizers, brain scientists, 3D programmers, mechanical engineers, materials scientists, artists, storytellers, and others.</p>

<h3 id="uncanny-valley">Uncanny Valley</h3>

<p>You need to find that level of the character that will work in AR, and also try to find the balance between how real your character is going to be, and how empathic you want to person feels of that character. Don’t fall down about your character.</p>

<p><img src="../../assets/figures/perception_ar/No.22(3)%20Storyboarding.jpg" alt="Image of No.22(3)" /></p>

<h3 id="summary">Summary</h3>

<p>AR UI is quite different from the traditional UI and requires new ways of approaching 
the challenge of informing and empowering users.</p>

  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">The Open Augmented Reality Teaching Book</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>The Open Augmented Reality Teaching Book</li>
          <li><a href="mailto:klamma@dbis.rwth-aachen.de">klamma@dbis.rwth-aachen.de</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li>
            <a href="https://twitter.com/AR_FOR_EU"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">AR_FOR_EU</span></a>

          </li>
          <li>
            <a href="/ar-for-eu-book/feed.xml"><span class="icon icon-rss"><?xml version="1.0"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"> 
<svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="128px" height="128px" id="RSSicon" viewBox="0 0 256 256">
<defs>
<linearGradient x1="0.085" y1="0.085" x2="0.915" y2="0.915" id="RSSg">
<stop  offset="0.0" stop-color="#E3702D"/><stop  offset="0.1071" stop-color="#EA7D31"/>
<stop  offset="0.3503" stop-color="#F69537"/><stop  offset="0.5" stop-color="#FB9E3A"/>
<stop  offset="0.7016" stop-color="#EA7C31"/><stop  offset="0.8866" stop-color="#DE642B"/>
<stop  offset="1.0" stop-color="#D95B29"/>
</linearGradient>
</defs>
<rect width="256" height="256" rx="55" ry="55" x="0"  y="0"  fill="#CC5D15"/>
<rect width="246" height="246" rx="50" ry="50" x="5"  y="5"  fill="#F49C52"/>
<rect width="236" height="236" rx="47" ry="47" x="10" y="10" fill="url(#RSSg)"/>
<circle cx="68" cy="189" r="24" fill="#FFF"/>
<path d="M160 213h-34a82 82 0 0 0 -82 -82v-34a116 116 0 0 1 116 116z" fill="#FFF"/>
<path d="M184 213A140 140 0 0 0 44 73 V 38a175 175 0 0 1 175 175z" fill="#FFF"/>
</svg>
</span><span class="username">Subscribe</span></a>

          </li>
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>The Open Augmented Reality Teaching Book</p>
      </div>
    </div>

  </div>

</footer>


    <!-- load visualizations (at the end so that the div elements are known here) -->
    
  </body>

</html>
