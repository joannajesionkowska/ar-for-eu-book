<!DOCTYPE html>
<html>

  ﻿<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Tracking Technologies (started)</title>
    <meta name="description" content="The Open Augmented Reality Teaching Book">

    <link rel="stylesheet" href="/ar-for-eu-book/css/main.css">
    <link rel="canonical" href="https://klamma.github.io/ar-for-eu-book/chapter/tracking/">
    <link rel="alternate" type="application/rss+xml" title="The Open Augmented Reality Teaching Book" href="https://klamma.github.io/ar-for-eu-book/feed.xml">

    <!-- Favicon head tag -->
    <link rel="shortcut icon" href="/ar-for-eu-book/favicon.ico" type="image/x-icon">

    <!-- Visualization Libraries -->
    <script src="https://code.jquery.com/jquery-3.1.1.min.js"></script>
    <script src="https://code.highcharts.com/highcharts.js"></script>
    <script src="https://code.highcharts.com/highcharts-more.js"></script>
    <script src="https://code.highcharts.com/modules/exporting.js"></script>

    <!-- formula rendering -->
    <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/ar-for-eu-book/">The Open Augmented Reality Teaching Book</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
	  <a class="page-link" href="https://codereality.net/">Code Reality</a>
          <a class="page-link" href="/ar-for-eu-book/posts">Blog Posts</a>
          <a class="page-link" href="/ar-for-eu-book/toc">Table of Content</a>
          <a class="page-link" href="/ar-for-eu-book/references">Bibliography</a>
          <a class="page-link" href="/ar-for-eu-book/contrib">Contributors</a>
          <a class="page-link" href="/ar-for-eu-book/about">About</a>
      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Tracking Technologies (started)</h1>
  </header>

  <div class="post-content">
    
<h1 id="tracking-technologies">Tracking Technologies</h1>

<p>Tracking technologies are required in order to provide the position and rotation of a real object to the computer system.
With AR systems, tracking technologies are commonly used in two settings:
If the system consists of a head-mounted display, e.g. the Microsoft HoloLens, tracking is required in order to capture the user’s head movements.
Advanced systems can also track other body parts like the hands.
In the second use-case, virtual objects should be attached to a real object.
In order to realize this, the position and rotation of the object relative to the camera must be determined.
An example for this is the marker tracking feature of smartphone-based AR, e.g. with Vuforia.
Marker-based tracking can usually be done with one camera.
The described techniques usually apply for more complex object tracking, e.g. of an HMD.</p>

<p>Different techniques and technologies are available for tracking.
Each of them have particular requirements, use-cases as well as advantages and disadvantages.</p>

<h2 id="light">Light</h2>

<p>One possibility is to use visible or invisible light for tracking objects.</p>

<h3 id="opto-electronical">Opto-Electronical</h3>

<p>With opto-electronical tracking, markers are placed on the target.
For detecting the markers, at least two (IR) cameras are placed around the subject.
Computer vision software can isolate the markers on the seen images and track their position on the 2D image <a class="citation" href="#Guer05">(Guerra-Filho, 2005)</a>.
Based on the known static position of the cameras, the 3D position of the marker can be reconstructed.
This is done by an algorithm which shoots virtual rays into the scene.
The rays start at the position of the camera and pass through the detected 2D point in the image.
Ideally, the intersection point of the rays from all cameras determines the 3D position of the marker.
However, due to setup or tracking inaccuracies, the rays will not exact exactly but pass each other with a small gap in-between.
In practice, the midpoint of this gap is calculated and the final position of the marker is determined to be the average of the midpoints between all rays.</p>

<p>Opto-electronical tracking can be found in an active and a passive setup.
Their main difference is the origin of the light which is detected by the camera.
Passive opto-electronical tracking uses markers with a special surface which is highly reflective for infrared light <a class="citation" href="#Guer05">(Guerra-Filho, 2005)</a>.
Infrared lamps placed in the surrounding point towards the target and illuminate the markers.
The reflection of the IR light will reach the cameras and this way, the markers become visible to the cameras.
In contrast to this, the markers in active opto-electronical tracking emit light, e.g. by using small infrared LEDs.
This means that no external light source is necessary.
The advantage of active opto-electronical tracking is that individual trackers can be turned on or off separately.
Leveraging this additional control, an initialization procedure can be developed where the system cannot just detect but also identify the markers.
For instance, with a full-body tracking, markers on the different on the hands can be distinguished.
A disadvantage of active opto-electronical tracking is that the markers require an energy source.
With passive tracking, it suffices to use plastic balls with a special surface paint as markers but with active tracking, cabling needs to be laid out and a battery needs to be mounted on the target.
This way, a passive tracking setup is non-intrusive.
In contrast to this, with active tracking the marker suit can be uncomfortable to wear and it can restrict movements due to the cables and battery.</p>

<p>The high accuracy and a low latency of opto-electronical tracking systems are beneficial.
However, occlusion can create a problem.
Since the technology is light-based, markers can only be tracked if they are visible to at least two cameras at once.
Challenges arise from the fact that one marker alone can only convey positional data but not the orientation of the target.
In order to reconstruct the orientation of an object, at least three markers are required.</p>

<p>One use-case for opto-electronical tracking is motion-tracking for movies or animations.
Actors are equipped with markers and perform movements on a stage which is surrounded by cameras.</p>

<h3 id="structured-light">Structured Light</h3>

<p>The technique of structured light tracking is for instance used by the Microsoft Kinect.
This technique requires a light source and a camera.
Both are typically operating in the infrared-range of light.
The light is occluded in parts so that it emits a characteristic pattern, e.g. stripes or a random point pattern <a class="citation" href="#ScSz03">(Scharstein &amp; Szeliski, 18-20 June 2003)</a>.
In a calibration procedure, the camera’s characteristics are determined regarding its mapping of a 3D point to a pixel in 2D <a class="citation" href="#Geng11">(Geng, 2011)</a>.
This is necessary since cameras can distort images.
As an example, wide-angle cameras show straight lines as curves.
Additionally, a calibration of the projector’s brightness and emission characteristics is necessary.
In the latter calibration step, the already calibrated camera views the projected pattern on a flat surface.
The position and rotation of the surface is known and by comparing the original pattern to the detected, undistorted pattern, one can calculate distortions which are introduced by the projector.</p>

<p>After that the tracking device can be used to determine the shape of surfaces.
This can be done by determining how the projected pattern is warped by the shape of the surface.
From this disturbance, depth information can be calculated and so 3D coordinates can be determined for all points in the camera image <a class="citation" href="#Geng11">(Geng, 2011)</a>.
In combination with shape recognition software, persons can be tracked.
The information can be used to overlay a virtual skeleton over a pose.
Thus, the technique can be used for motion capture.
Additionally, it is also used for 3D scanning, e.g. with the Microsoft HoloLens to create a 3D model of the environment.
An advantage for 3D scanning is that the technique can be combined with a standard camera which captures visible light.
This way, color information can be captured and applied to a scanned model as a texture.</p>

<figure class="image">
    <img src="../../assets/figures/tracking/StructuredLight.png" alt="Structured light which is projected down from above reveals the shape of the half-sphere" />
    <figcaption>Structured light which is projected down from above reveals the shape of the half-sphere</figcaption>
</figure>

<h3 id="rgb-cameras">RGB Cameras</h3>

<p>Video tracking uses a standard RGB camera feed as the information source for tracking.
There are two alternative setups possible.
The camera can be used to track another object or person in the video feed or the changes between frames can be used to determine the position and rotation of the camera in the environment.</p>

<p>Camera-based tracking is used for photogrammetry and marker-based AR.
In both cases, the tracking application looks for distinct feature points in the image and tries to locate them in each frame.
If the camera is moved, the depth of these feature points can be determined from the parallax effect between the two frames.</p>

<h3 id="time-of-flight-sensors">Time-of-Flight Sensors</h3>

<p>Time-of-Flight Sensors use the speed of light to determine how far an object is away from the sensor.
Unlike laser scanning sensors which can only scan single points at once, Time-of-Flight sensors are able to provide a depth map for a (low-resolution) image.
The sensor consists of a light source which emits a pulse of light, e.g. infrared light.
The light is reflected in the environment and captured by an image sensor.
From the difference in time between the emission and the received reflection, the distance to the reflecting surface can be computed.
Due to the high speed of light, the measurement logic needs to be very quick, i.e. in the range of picoseconds.</p>

<h2 id="ultra-sound">Ultra Sound</h2>

<h2 id="mechanical">Mechanical</h2>

<p>With mechanical tracking setups, movements can be tracked by their effect on a mechanical system.
Examples for mechanical tracking systems include exoskeletons.
This tracking technique was also used on the Ultimate Display where the head-mounted display was connected to a series of metal bars.
By moving the head, the bars would be pushed out of the way or pulled along and could rotate in a mechanical construction.
This rotation was registered by the system and from the amount of rotation the head movement could be reconstructed.
In modern systems, mechanical tracking can be found in combination with haptic feedback devices.
One example is the PHANToM haptic device.
It provides a pen-like handle to the user which is attached to a robotic arm.
The user can grab this pen and move it around.
The robotic arm can register the movements and passes the movement data on the application.
Haptic feedback is achieved by moving the robotic arm.
This provides forces on the pen which hinder movements in certain directions.</p>

<h2 id="electromagnetic">Electromagnetic</h2>

<p>Another way of tracking uses electromagnetism.
Here, the senors on the tracked object are situated in an alternating electromagnetic field.
A sensor consists of different coils, each of them is wound around one of the three axes.
If the sensor is moved in the field, a current is induced on each of the coils.
By measuring the strength of the current, the rotation and position of the sensor can be derived.</p>

<p>Unlike light-based tracking, electromagnetic tracking is immune to target occlusion, i.e. the tracker does not need to be in the line of sight of some external sensor.
However, the large sensors of 1-2cm are a disadvantage of this technique.
Additionally, the trackers require cabling.
One also needs to be careful with ferromagnetic elements which can influence the magnetic field and can therefore invalidate the tracking results.</p>

<h2 id="inertial">Inertial</h2>

<p>An Inertial Measurement Unit (IMU) is a sensor that reports - typically (but sometimes only a subset) - acceleration, rotation speed, and orientation from an accelerometer, gyroscope, and (not always) a magnetometer. Applications of IMUs are for motion capture or as input modality in interaction. The IMU in your mobile phone allows your maps application to function when the GPS signal is not available, for example while driving in a tunnel or when inside of a building. Most modern delivery devices have built in IMUs (smart glasses, mobile phones, tablet computers) or are used in combination with the delivery system (like with pico projectors). All IMUs inevitably suffer of drift, some of them more, some of them less, typically then using on board or software sensor fusion to reduce its amount. The software interface to the inertial measurement unit provides typically a quaternion for orientation (or Euler angles), and vectors for velocity and acceleration. These can be directly mapped in 3D environments to control objects (as a direct-mapping interaction device) or the viewing camera renderer (for scene exploration, think smart glasses).</p>

<h1 id="setups">Setups</h1>

<p>Tracking systems can be set up in two ways.
On the one hand, the sensors can be placed in the environment and they track markers on a moving object.
On the other hand, the sensors can be mounted to the moving target while the recognizable features are situated in the environment.</p>

<h2 id="outside-in-tracking">Outside-In Tracking</h2>

<h2 id="inside-out-tracking">Inside-Out Tracking</h2>

  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">The Open Augmented Reality Teaching Book</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>The Open Augmented Reality Teaching Book</li>
          <li><a href="mailto:klamma@dbis.rwth-aachen.de">klamma@dbis.rwth-aachen.de</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li>
            <a href="https://twitter.com/AR_FOR_EU"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">AR_FOR_EU</span></a>

          </li>
          <li>
            <a href="/ar-for-eu-book/feed.xml"><span class="icon icon-rss"><?xml version="1.0"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"> 
<svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="128px" height="128px" id="RSSicon" viewBox="0 0 256 256">
<defs>
<linearGradient x1="0.085" y1="0.085" x2="0.915" y2="0.915" id="RSSg">
<stop  offset="0.0" stop-color="#E3702D"/><stop  offset="0.1071" stop-color="#EA7D31"/>
<stop  offset="0.3503" stop-color="#F69537"/><stop  offset="0.5" stop-color="#FB9E3A"/>
<stop  offset="0.7016" stop-color="#EA7C31"/><stop  offset="0.8866" stop-color="#DE642B"/>
<stop  offset="1.0" stop-color="#D95B29"/>
</linearGradient>
</defs>
<rect width="256" height="256" rx="55" ry="55" x="0"  y="0"  fill="#CC5D15"/>
<rect width="246" height="246" rx="50" ry="50" x="5"  y="5"  fill="#F49C52"/>
<rect width="236" height="236" rx="47" ry="47" x="10" y="10" fill="url(#RSSg)"/>
<circle cx="68" cy="189" r="24" fill="#FFF"/>
<path d="M160 213h-34a82 82 0 0 0 -82 -82v-34a116 116 0 0 1 116 116z" fill="#FFF"/>
<path d="M184 213A140 140 0 0 0 44 73 V 38a175 175 0 0 1 175 175z" fill="#FFF"/>
</svg>
</span><span class="username">Subscribe</span></a>

          </li>
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>The Open Augmented Reality Teaching Book</p>
      </div>
    </div>

  </div>

</footer>


    <!-- load visualizations (at the end so that the div elements are known here) -->
    
  </body>

</html>
